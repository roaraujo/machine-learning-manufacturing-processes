{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import ks_2samp\n",
    "from IPython.display import Image\n",
    "\n",
    "import shap\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, LeaveOneOut, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "\n",
    "import joblib\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "# from plot_learning import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn_rvm import EMRVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, RandomizedSearchCV\n",
    "from sklearn.metrics  import average_precision_score, make_scorer, roc_curve,f1_score, precision_score, recall_score, fbeta_score, auc, roc_auc_score, accuracy_score, confusion_matrix, classification_report,precision_recall_curve\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "from sklearn_genetic import GASearchCV\n",
    "from sklearn_genetic.space import Continuous, Categorical, Integer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "train = pd.read_csv(\"train.csv\", sep = \";\")\n",
    "test = pd.read_csv(\"test.csv\", sep = \";\")\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARS = ['CONICITY', 'RFV','RRO', 'H2RFV',\n",
    "        'PLY','LFV', 'CAPSPLICE']\n",
    "# VARS = ['CONICITY', 'RRO', 'H2RFV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Y2'] = train['Y'].apply(lambda x: 1 if x == 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_test, y_pred):\n",
    "    cm = list()\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_df = pd.DataFrame(cm)                      \n",
    "    plt.figure(figsize=(8,6))  \n",
    "    sns.heatmap(cm_df, annot=True)\n",
    "    print(\"Classification Report: \\n\", classification_report(y_test, y_pred))  \n",
    "\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, multi_class = 'ovr', average=None)\n",
    "    gini = 2*roc_auc -1\n",
    "    print(\"Gini: \",gini)\n",
    "    print(\"ROC AUC:: \",roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train[train['Y']==0]\n",
    "train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "from pyod.models.inne import INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyecod = ECOD(contamination=0.5,n_jobs=-1)\n",
    "# pyecod = INNE(contamination=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyecod.fit(train2[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['scores_pyecod']=pyecod.decision_function(test[VARS])\n",
    "test['pyecod']=pyecod.predict(test[VARS])\n",
    "# test['pyecod'] = test['pyecod'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['scores_pyecod']=pyecod.decision_function(train[VARS])\n",
    "train['pyecod']=pyecod.predict(train[VARS])\n",
    "# train['pyecod'] = train['pyecod'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=test, x=\"scores_pyecod\", hue=test.Y,\n",
    "    cumulative=True, common_norm=False, common_grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(test.Y, test.pyecod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(train.Y, train.pyecod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IsolationForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=65)\n",
    "\n",
    "# cv = LeaveOneOut()\n",
    "\n",
    "forest_s = IsolationForest()\n",
    "f1sc = make_scorer(f1_score)\n",
    "\n",
    "ps = {\"n_estimators\": (100, 256), \n",
    "      \"max_features\": (0.5, 0.7, 0.8, 0.9, 1.0),\n",
    "      \"contamination\":(0.49, 0.5)}\n",
    "search = GridSearchCV(estimator=forest_s, param_grid=ps, scoring='recall', cv=cv)\n",
    "search.fit(train[VARS], train.Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(search.cv_results_)\n",
    "df[[\"mean_test_score\", \"std_test_score\", \"params\"]].sort_values(by=[\"mean_test_score\"], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest=IsolationForest(n_estimators=256, max_samples='auto',max_features=0.9, contamination = 0.5, random_state=75)\n",
    "iforest.fit(train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['scores_forest']=iforest.decision_function(test[VARS])\n",
    "test['iForest']=iforest.predict(test[VARS])\n",
    "test['iForest'] = test['iForest'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['scores_forest']=iforest.decision_function(train[VARS])\n",
    "train['iForest']=iforest.predict(train[VARS])\n",
    "train['iForest'] = train['iForest'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.Y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=test, x=\"scores_forest\", hue=test.Y,\n",
    "    cumulative=True, common_norm=False, common_grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(test.Y, test.iForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(17)/(17+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(train.Y, train.iForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = shap.TreeExplainer(iforest) #Explainer\n",
    "shap_values = exp.shap_values(train[VARS])  #Calculate SHAP values\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train[VARS],plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(exp.expected_value, shap_values[2],features =train[VARS].iloc[2,:] ,feature_names =train[VARS].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.bar_plot(shap_values[2],features =train[VARS].iloc[2,:] ,feature_names =train[VARS].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoEnconder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyod.models.auto_encoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = AutoEncoder(hidden_neurons =[25, 2, 2, 25])\n",
    "# clf.fit(train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the outlier scores for the train data\n",
    "# y_train_scores = clf.decision_scores_\n",
    "# # Predict the anomaly scores\n",
    "# y_test_scores = clf.decision_function(test[VARS])  # outlier scores\n",
    "# y_test_scores = pd.Series(y_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot it!\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(y_test_scores, bins='auto')\n",
    "# plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = test[VARS].copy()\n",
    "# df_test['score'] = y_test_scores\n",
    "# df_test['cluster'] = np.where(df_test['score']<2.5, 0, 1)\n",
    "# df_test['cluster'].value_counts()\n",
    "# df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.crosstab(test.Y, df_test['cluster'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneClassSVM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.svm import OneClassSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm = OneClassSVM(gamma='scale', nu=0.5, kernel='rbf')\n",
    "ocsvm.fit(train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['OCSVM']=ocsvm.predict(train[VARS])\n",
    "train['score_OCSVM']=ocsvm.decision_function(train[VARS])\n",
    "train['OCSVM'] = train['OCSVM'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['OCSVM']=ocsvm.predict(test[VARS])\n",
    "test['score_OCSVM']=ocsvm.decision_function(test[VARS])\n",
    "test['OCSVM'] = test['OCSVM'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=test, x=\"score_OCSVM\", hue=test.Y,\n",
    "    cumulative=True, common_norm=False, common_grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(test.Y, test.OCSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(train.Y, train.OCSVM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LocalOutlierFactor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof = LocalOutlierFactor(n_neighbors=4, novelty=True, p=2, metric = 'minkowski', contamination=0.5)\n",
    "lof.fit(train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LOF']=lof.predict(train[VARS])\n",
    "train['score_LOF']=lof.decision_function(train[VARS])\n",
    "train['LOF'] = train['LOF'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['LOF']=lof.predict(test[VARS])\n",
    "test['score_LOF']=lof.decision_function(test[VARS])\n",
    "test['LOF'] = test['LOF'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=test, x=\"score_LOF\", hue=test.Y,\n",
    "    cumulative=True, common_norm=False, common_grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(test.Y, test.LOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(train.Y, train.LOF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EllipticEnvelope"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ell = EllipticEnvelope(contamination=0.3)\n",
    "ell.fit(train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['EE']=ell.predict(train[VARS])\n",
    "train['score_EE']=ell.decision_function(train[VARS])\n",
    "train['EE'] = train['EE'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['EE']=ell.predict(test[VARS])\n",
    "test['score_EE']=ell.decision_function(test[VARS])\n",
    "test['EE'] = test['EE'].apply(lambda x: 1 if x<0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=test, x=\"score_EE\", hue=test.Y,\n",
    "    cumulative=True, common_norm=False, common_grid=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = confusion_matrix(test.Y, test.EE)\n",
    "# cm_df = pd.DataFrame(cm)                      \n",
    "# plt.figure(figsize=(8,6))  \n",
    "# sns.heatmap(cm_df, annot=True)\n",
    "# print(\"Classification Report: \\n\", classification_report(test.Y, test.EE))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(test.Y, test.EE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(train.Y, train.EE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def plot_roc(y_pred, y_test, l):\n",
    "\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_test = np.array(y_test)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, y_pred, multi_class = 'ovr', average=None)\n",
    "\n",
    "    \n",
    "    auc = round(metrics.roc_auc_score(y_test, y_pred), 3)\n",
    "    plt.plot(fpr,tpr,label=l+ \" , AUC=\"+str(auc))\n",
    "    plt.xlabel('Taxa falsos positivos')\n",
    "    plt.ylabel('Taxa verdadeiros positivos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['OCSVM', 'iForest', \"EE\", \"LOF\"]\n",
    "\n",
    "#set up plotting area\n",
    "plt.figure(0).clf()\n",
    "\n",
    "for i, j in enumerate(labels):\n",
    "\n",
    "    plot_roc(test[j],test.Y, labels[i])\n",
    "\n",
    "\n",
    "#add legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all classifier results\n",
    "ensemble = pd.concat([test.iForest, test.OCSVM, test.LOF, test.EE],axis=1)\n",
    "\n",
    "\n",
    "# g= sns.heatmap(ensemble.corr(),annot=True)\n",
    "\n",
    "\n",
    "corrmat = np.triu(ensemble.corr(method='spearman'))\n",
    "\n",
    "g = sns.heatmap(ensemble.corr(method='spearman'),cmap=\"coolwarm\",annot=True, mask = corrmat, fmt=\".2f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(c1, c2, c3, c4):\n",
    "    if c1==1 and c2==1:\n",
    "        return 1\n",
    "    elif c3==1 and c2==1:\n",
    "        return 1\n",
    "    elif c3==1 and c1==1:\n",
    "        return 1\n",
    "    elif c3==1 and c4==1:\n",
    "        return 1\n",
    "    elif c2==1 and c4==1:\n",
    "        return 1\n",
    "    elif c1==1 and c4==1:\n",
    "        return 1\n",
    "    elif c1==1 and c2==1 and c3==1:\n",
    "        return 1\n",
    "    elif c1==1 and c2==1 and c3==1 and c4==1:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "test['Ensemble'] = test.apply(lambda row: voting(row['iForest'], row['OCSVM'], row['LOF'], row['EE']), axis=1)\n",
    "test['Ensemble'] = test['Ensemble'].astype(int)\n",
    "\n",
    "train['Ensemble'] = train.apply(lambda row: voting(row['iForest'], row['OCSVM'], row['LOF'], row['EE']), axis=1)\n",
    "train['Ensemble'] = train['Ensemble'].astype(int)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test.Y, test.Ensemble)\n",
    "cm_df = pd.DataFrame(cm)                      \n",
    "plt.figure(figsize=(8,6))  \n",
    "sns.heatmap(cm_df, annot=True)\n",
    "print(\"Classification Report: \\n\", classification_report(test.Y, test.Ensemble))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(test.Y, test.Ensemble, multi_class = 'ovr', average=None)\n",
    "gini = 2*roc_auc -1\n",
    "print(\"Gini: \",gini)\n",
    "print(\"ROC AUC:: \",roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['OCSVM', 'iForest', 'Ensemble', \"EE\", \"LOF\"]\n",
    "#set up plotting area\n",
    "plt.figure(0).clf()\n",
    "\n",
    "for i, j in enumerate(labels):\n",
    "\n",
    "    plot_roc(test[j],test.Y, labels[i])\n",
    "\n",
    "\n",
    "#add legend\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McNemar’s Test for Classifiers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fail to Reject Null Hypothesis: Classifiers have a similar proportion of errors on the test set. \n",
    "\n",
    "Reject Null Hypothesis: Classifiers have a different proportion of errors on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test.LOF, test.iForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = mcnemar_table(y_target=test.Y, \n",
    "                   y_model1=test.LOF, \n",
    "                   y_model2=test.iForest)\n",
    "\n",
    "table1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test.OCSVM, test.iForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = mcnemar_table(y_target=test.Y, \n",
    "                   y_model1=test.OCSVM, \n",
    "                   y_model2=test.iForest)\n",
    "\n",
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test.LOF, test.OCSVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table3 = mcnemar_table(y_target=test.Y, \n",
    "                   y_model1=test.LOF, \n",
    "                   y_model2=test.OCSVM)\n",
    "\n",
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test.LOF, test.EE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table4 = mcnemar_table(y_target=test.Y, \n",
    "                   y_model1=test.LOF, \n",
    "                   y_model2=test.EE)\n",
    "table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test.OCSVM, test.EE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table5 = mcnemar_table(y_target=test.Y, \n",
    "                   y_model1=test.OCSVM, \n",
    "                   y_model2=test.EE)\n",
    "\n",
    "table5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(test.EE, test.iForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table6 = mcnemar_table(y_target=test.Y, \n",
    "                   y_model1=test.EE, \n",
    "                   y_model2=test.iForest)\n",
    "\n",
    "table6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hip(table):\n",
    "    # calculate mcnemar test\n",
    "    result = mcnemar(table, exact=True,  correction=True)\n",
    "    # summarize the finding\n",
    "    print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "    # interpret the p-value\n",
    "    alpha = 0.05\n",
    "    if result.pvalue > alpha:\n",
    "        print('Same proportions of errors (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different proportions of errors (reject H0)')\n",
    "\n",
    "\n",
    "def test_hip2(table):\n",
    "    result = mcnemar(table, exact=True)\n",
    "    p = result.pvalue\n",
    "    return p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('LOF vs IsolationForest')\n",
    "test_hip(table1)\n",
    "print('OCSVM vs IsolationForest')\n",
    "test_hip(table2)\n",
    "print('LOF vs OCSVM')\n",
    "test_hip(table3)\n",
    "print('LOF vs EE')\n",
    "test_hip(table4)\n",
    "print('OCSVM vs EE')\n",
    "test_hip(table5)\n",
    "print('IsolationForest vs EE')\n",
    "test_hip(table6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value = list()\n",
    "tables = [table1, table2, table3, table4, table5, table6]\n",
    "for i in tables:\n",
    "    p_value.append(test_hip2(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_scatter(features, labels, dimensions=2, save_as='graph.png'):\n",
    "    if dimensions not in (2, 3):\n",
    "        raise ValueError('tsne_scatter can only plot in 2d or 3d (What are you? An alien that can visualise >3d?). Make sure the \"dimensions\" argument is in (2, 3)')\n",
    "\n",
    "    # t-SNE dimensionality reduction\n",
    "    features_embedded = TSNE(n_components=dimensions, random_state=23).fit_transform(features)\n",
    "    \n",
    "    # initialising the plot\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    \n",
    "    # counting dimensions\n",
    "    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # plotting data\n",
    "    ax.scatter(\n",
    "        *zip(*features_embedded[np.where(labels==1)]),\n",
    "        marker='o',\n",
    "        color='r',\n",
    "        s=2,\n",
    "        alpha=0.7,\n",
    "        label=1\n",
    "    )\n",
    "    ax.scatter(\n",
    "        *zip(*features_embedded[np.where(labels==0)]),\n",
    "        marker='o',\n",
    "        color='g',\n",
    "        s=2,\n",
    "        alpha=0.3,\n",
    "        label=0\n",
    "    )\n",
    "\n",
    "    # storing it to be displayed later\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(save_as);\n",
    "    plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "tsne = TSNE(n_components)\n",
    "tsne_result = tsne.fit_transform(train[VARS])\n",
    "tsne_result.shape\n",
    "# (1000, 2)\n",
    "# Two dimensions for each of our images\n",
    " \n",
    "# Plot the result of our TSNE with the label color coded\n",
    "# A lot of the stuff here is about making the plot look pretty and not TSNE\n",
    "tsne_result_df = pd.DataFrame({'tsne_1': tsne_result[:,0], 'tsne_2': tsne_result[:,1], 'label': train.Y})\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_result_df, ax=ax,s=120)\n",
    "lim = (tsne_result.min()-5, tsne_result.max()+5)\n",
    "ax.set_xlim(lim)\n",
    "ax.set_ylim(lim)\n",
    "ax.set_aspect('equal')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CONJUNTO'] = 'train'\n",
    "test['CONJUNTO'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train, test], axis=0)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, verbose=1, perplexity=30, n_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduce = tsne.fit_transform(df[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result_df = pd.DataFrame({'tsne_1': X_reduce[:,0], 'tsne_2': X_reduce[:,1], 'tsne_3': X_reduce[:,0]})\n",
    "\n",
    "tsne_result_df = tsne_result_df.reset_index(drop=True)\n",
    "data = df.reset_index(drop=True)\n",
    "\n",
    "data2 = pd.concat([data, tsne_result_df], axis=1)\n",
    "# data2= data2[data2['CONJUNTO']=='test']\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snips as snp  # my snippets\n",
    "# snp.prettyplot(matplotlib)  # my aesthetic preferences for plotting\n",
    "# %matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_zlabel(\"tSNE_3\")\n",
    "ax.set_ylabel(\"tSNE_2\")\n",
    "ax.set_xlabel(\"tSNE_1\")\n",
    "# Plot the reduced dimensionality data points\n",
    "ax.scatter(X_reduce[:,0], X_reduce[:,1], zs=X_reduce[:,2], s=10, lw=2, c='blue')\n",
    "\n",
    "# Plot circles around the predicted outliers\n",
    "ax.scatter(X_reduce[data2.iForest==1, 0], X_reduce[data2.iForest==1, 1], zs=X_reduce[data2.iForest==1, 2], \n",
    "           lw=2, facecolors=\"none\", edgecolors=\"red\", s=80, label=\"anomalia\")\n",
    "\n",
    "# Plot x's for the ground truth outliers\n",
    "ax.scatter(X_reduce[data2.Y==1, 0], X_reduce[data2.Y==1, 1], zs=X_reduce[data2.Y==1, 2], \n",
    "           lw=2, s=50, marker=\"x\", c=\"red\", label=\"outlier\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne2 = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduce2 = tsne2.fit_transform(train[VARS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_result_df2 = pd.DataFrame({'tsne_1': X_reduce2[:,0], 'tsne_2': X_reduce2[:,1]})\n",
    "\n",
    "tsne_result_df2 = tsne_result_df2.reset_index(drop=True)\n",
    "data3 = train.reset_index(drop=True)\n",
    "\n",
    "data4 = pd.concat([data3, tsne_result_df2], axis=1)\n",
    "data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=data4, x=\"tsne_1\", y=\"tsne_2\", hue=\"iForest\", fill=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('artigo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "77b285dce526106f0619df26edb004dd0a55300a7a7a7660bd0bf561a0915729"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
